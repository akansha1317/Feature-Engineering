{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Feature Engineering***"
      ],
      "metadata": {
        "id": "chXBAYjeHCIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "- Certainly! Here's a theoretical definition of **parameter**:\n",
        "\n",
        "---\n",
        "\n",
        "### **Theoretical Definition of a Parameter**:\n",
        "\n",
        "A **parameter** is a symbolic representation of a constant value that characterizes a system, model, or function. It is typically not the main variable of interest but rather a defining element that influences the behavior or outcome of a process or equation.\n",
        "\n",
        "In theoretical terms:\n",
        "\n",
        "* A parameter remains fixed during a particular analysis but can vary across different scenarios.\n",
        "* It serves to define a family of functions, equations, or models.\n",
        "* Unlike variables, which are the focus of evaluation and manipulation, parameters provide **structure** or **constraints** within which variables operate.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Characteristics**:\n",
        "\n",
        "1. **Fixed in context**: Within a given scenario or problem, a parameter is usually treated as constant.\n",
        "2. **Defines behavior**: It affects how a function or system responds, without being directly manipulated.\n",
        "3. **Abstract role**: In theory, parameters are used to generalize results or to describe a class of problems or systems.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example in Theory**:\n",
        "\n",
        "In the function $f(x) = ax^2 + bx + c$,\n",
        "\n",
        "* $x$ is the **independent variable**.\n",
        "* $a$, $b$, and $c$ are **parameters**—they define the shape and position of the parabola but are not themselves variables in the function.\n"
      ],
      "metadata": {
        "id": "Q4FAkpHuHQ5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "- ### 🔹 What is **Correlation**?\n",
        "\n",
        "**Correlation** is a statistical concept that measures the **strength** and **direction** of a relationship between two variables. It answers the question:\n",
        "\n",
        "> *\"When one variable changes, does the other tend to change in a predictable way?\"*\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Key Points:\n",
        "\n",
        "* **Range**: Correlation is usually measured with the **correlation coefficient** $r$, which ranges from **–1 to +1**.\n",
        "* **Interpretation of $r$**:\n",
        "\n",
        "  * $r = +1$: Perfect **positive** correlation (both variables move in the same direction).\n",
        "  * $r = -1$: Perfect **negative** correlation (variables move in opposite directions).\n",
        "  * $r = 0$: **No correlation** (no predictable relationship).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 What Does **Negative Correlation** Mean?\n",
        "\n",
        "A **negative correlation** means that as one variable **increases**, the other **decreases**, and vice versa. The relationship is **inverse**.\n",
        "\n",
        "#### Examples:\n",
        "\n",
        "* The more time you **spend watching TV**, the less time you **have for studying**.\n",
        "* As the **price of a product** increases, **demand** usually decreases (basic economics).\n",
        "* In finance, some assets (e.g. gold and stocks) often show negative correlation: when stock prices fall, gold prices might rise.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dZICrJUWHqZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "- ### 🔹 **Definition of Machine Learning (ML):**\n",
        "\n",
        "**Machine Learning** is a field of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to **learn patterns from data** and **make decisions or predictions** without being explicitly programmed for specific tasks.\n",
        "\n",
        "> In theory, machine learning is the process by which a system **improves its performance on a task** through **experience** (data).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Main Components of Machine Learning:**\n",
        "\n",
        "Machine Learning systems are typically composed of several key components. These can be grouped into **four main categories**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Data**\n",
        "\n",
        "* The foundation of machine learning.\n",
        "* Can be **labeled** (supervised learning) or **unlabeled** (unsupervised learning).\n",
        "* Must be preprocessed (cleaned, normalized, etc.) before use.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Model**\n",
        "\n",
        "* A **mathematical or computational representation** that learns patterns from the data.\n",
        "* Examples: Linear regression, decision trees, neural networks.\n",
        "* The model has **parameters** that are adjusted during training.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Learning Algorithm**\n",
        "\n",
        "* The method used to **train the model** on data by adjusting its parameters to reduce errors.\n",
        "* It minimizes a **loss function** (a measure of prediction error).\n",
        "* Examples: Gradient descent, backpropagation (in neural networks), k-means clustering.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Prediction / Inference**\n",
        "\n",
        "* Once trained, the model can take **new, unseen inputs** and produce **predictions or decisions**.\n",
        "* The model’s usefulness is tested on **validation** and **test data**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kiub5X8gH49B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "- Certainly. Here's a **theoretical explanation**:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Loss Value in Theory**\n",
        "\n",
        "In machine learning theory, the **loss function** is a formal mathematical function that quantifies the **error** between the predicted output of a model and the actual output (true label). The **loss value** is the result of this function for a given prediction.\n",
        "\n",
        "> **The loss value serves as a scalar representation of how far off a prediction is from the ground truth.**\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Role of Loss in Model Evaluation (Theoretical View):\n",
        "\n",
        "1. **Objective Function**:\n",
        "   In theory, training a machine learning model is an **optimization problem**. The goal is to **minimize a loss function**, which serves as the **objective function**.\n",
        "\n",
        "   $$\n",
        "   \\min_{\\theta} \\; L(f_\\theta(x), y)\n",
        "   $$\n",
        "\n",
        "   Here:\n",
        "\n",
        "   * $f_\\theta(x)$ is the model’s prediction (parameterized by $\\theta$)\n",
        "   * $y$ is the true value\n",
        "   * $L$ is the loss function\n",
        "\n",
        "2. **Indicator of Learning Progress**:\n",
        "   A decreasing loss value during training indicates that the model is moving toward a **hypothesis function** that better fits the data.\n",
        "\n",
        "3. **Proxy for Risk**:\n",
        "   The expected loss over all possible data points (called **expected risk**) is the theoretical measure of model performance:\n",
        "\n",
        "   $$\n",
        "   R(\\theta) = \\mathbb{E}_{(x, y) \\sim P} [L(f_\\theta(x), y)]\n",
        "   $$\n",
        "\n",
        "   This is often approximated using empirical risk:\n",
        "\n",
        "   $$\n",
        "   \\hat{R}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(f_\\theta(x_i), y_i)\n",
        "   $$\n",
        "\n",
        "4. **Model Comparison**:\n",
        "   In theoretical analysis, models are often compared based on their loss values. A model with a **lower theoretical risk** (or loss) is considered **better** in terms of generalization under ideal conditions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f48ErX3aILnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        " - ### 🔹 **Continuous and Categorical Variables – Theoretical Definitions**\n",
        "\n",
        "In statistics and machine learning, **variables** represent characteristics or properties that can take on different values. They are typically classified into two main types: **continuous** and **categorical**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 **1. Continuous Variables**\n",
        "\n",
        "A **continuous variable** is a variable that can take **any numerical value** within a given range. Theoretically, it can assume **an infinite number of possible values** between any two values.\n",
        "\n",
        "#### **Theoretical Characteristics**:\n",
        "\n",
        "* Values are drawn from an **interval** or **real number space**.\n",
        "* The set of possible values is **uncountably infinite**.\n",
        "* The variable has a **numerical scale** with meaningful ordering and distance.\n",
        "\n",
        "#### **Examples**:\n",
        "\n",
        "* Height (e.g., 172.3 cm)\n",
        "* Temperature (e.g., 36.6°C)\n",
        "* Time (e.g., 3.14159 seconds)\n",
        "* Weight, speed, age (in precise units)\n",
        "\n",
        "#### **Mathematical Note**:\n",
        "\n",
        "If $X$ is a continuous variable, then $P(X = x) = 0$ for any single value $x$; probabilities are defined over **intervals**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 **2. Categorical Variables**\n",
        "\n",
        "A **categorical variable** is a variable that takes on **a limited number of distinct categories** or **labels**. These categories represent **qualitative** differences rather than numerical values.\n",
        "\n",
        "#### **Theoretical Characteristics**:\n",
        "\n",
        "* Values belong to a **finite or countably infinite set**.\n",
        "* The values represent **types**, **groups**, or **class labels**.\n",
        "* There is **no inherent numeric meaning** in the categories (except when ordered).\n",
        "\n",
        "#### **Subtypes**:\n",
        "\n",
        "1. **Nominal** – Categories without any natural order (e.g., colors: red, blue, green).\n",
        "2. **Ordinal** – Categories with an implicit order (e.g., rating: poor, fair, good, excellent).\n",
        "\n",
        "#### **Examples**:\n",
        "\n",
        "* Gender (male, female, non-binary)\n",
        "* Country (USA, India, Japan)\n",
        "* Animal species (cat, dog, bird)\n",
        "* Education level (ordinal: high school, bachelor’s, master’s)\n",
        "\n"
      ],
      "metadata": {
        "id": "GUEu0bnrImmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        " - Certainly. Here's a **theoretical explanation** of how categorical variables are handled in machine learning:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Handling Categorical Variables in Machine Learning — Theoretical Perspective**\n",
        "\n",
        "In the theory of machine learning, models operate within a **numerical vector space**. However, **categorical variables** are inherently **non-numeric** and belong to a **finite or countable discrete set**. Thus, to integrate them into a learning algorithm, they must be **transformed into a numerical representation** that retains as much of their **informational content** as possible, without introducing misleading structure.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 **Theoretical Techniques for Encoding Categorical Variables**\n",
        "\n",
        "#### **1. Label Encoding (Nominal-to-Ordinal Mapping)**\n",
        "\n",
        "Label encoding maps each category to a unique integer:\n",
        "\n",
        "$$\n",
        "\\text{Category } c_i \\in \\{c_1, c_2, ..., c_k\\} \\rightarrow \\text{Integer } i \\in \\{0, 1, ..., k-1\\}\n",
        "$$\n",
        "\n",
        "* **Theoretical Benefit**: Provides a simple embedding of categories into a discrete numeric space.\n",
        "* **Theoretical Risk**: Imposes an **artificial order** or metric structure on nominal data, which may **bias algorithms** that assume distance or continuity (e.g., linear regression, k-nearest neighbors).\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. One-Hot Encoding (Basis Vector Representation)**\n",
        "\n",
        "Each category is represented as a **unit vector** in a $k$-dimensional binary space:\n",
        "\n",
        "$$\n",
        "\\text{Category } c_i \\rightarrow \\mathbf{e}_i \\in \\{0,1\\}^k \\text{ where } \\mathbf{e}_i[i] = 1\n",
        "$$\n",
        "\n",
        "* **Theoretical Benefit**: Preserves **pure category identity** without implying order or distance.\n",
        "* **Theoretical Limitation**: Results in **high-dimensional sparse vectors**, which may increase computational complexity and overfitting risk in high-cardinality settings.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Ordinal Encoding (Ordered Embedding)**\n",
        "\n",
        "Applies only to **ordinal categorical variables**, where a natural ordering exists:\n",
        "\n",
        "$$\n",
        "c_1 < c_2 < \\dots < c_k \\rightarrow \\text{Integer values reflecting this order}\n",
        "$$\n",
        "\n",
        "* **Theoretical Justification**: Embeds the categorical variable into an ordered space, preserving meaningful rank relations.\n",
        "* **Caution**: Should not be applied to **nominal variables**, as it may introduce **false ordinal relationships**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Target Encoding (Supervised Mapping)**\n",
        "\n",
        "Each category is encoded using the **expected value of the target variable**, conditioned on that category:\n",
        "\n",
        "$$\n",
        "c_i \\rightarrow \\mathbb{E}[Y \\mid X = c_i]\n",
        "$$\n",
        "\n",
        "* **Theoretical Basis**: Uses **Bayesian or statistical estimation** to encode categories based on their **predictive relationship** with the outcome.\n",
        "* **Theoretical Risk**: May cause **data leakage** or **overfitting** unless properly regularized or applied with cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Embedding Methods (Learned Representations)**\n",
        "\n",
        "Especially in deep learning, categories are mapped to vectors in a **learned continuous vector space**:\n",
        "\n",
        "$$\n",
        "c_i \\rightarrow \\mathbf{v}_i \\in \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "* **Theoretical Idea**: The embedding vectors are treated as **latent variables**, optimized jointly with model parameters to capture **semantic similarity** or **functional utility**.\n",
        "* Embeddings can represent **context-dependent meaning** and **category similarity** in high-dimensional space.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hjgxIij7J_yJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "- ### 🔹 **What Is Meant by Training and Testing a Dataset?** *(Theoretical Explanation)*\n",
        "\n",
        "In machine learning theory, the process of building a predictive model involves **learning patterns** from data and then **evaluating** how well those patterns generalize to new, unseen data. This is achieved through **splitting the dataset** into two main parts: **training** and **testing** sets.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 **1. Training a Dataset (Training Set)**\n",
        "\n",
        "> **Definition**: The **training set** is the subset of data used to **fit** the machine learning model.\n",
        "> The model **learns parameters** or patterns by minimizing a loss function on this data.\n",
        "\n",
        "#### **Theoretical View**:\n",
        "\n",
        "* Training involves solving an **optimization problem**:\n",
        "\n",
        "  $$\n",
        "  \\min_{\\theta} \\; \\frac{1}{n} \\sum_{i=1}^{n} L(f_\\theta(x_i), y_i)\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "\n",
        "  * $f_\\theta$ is the model function with parameters $\\theta$\n",
        "  * $(x_i, y_i)$ are training examples\n",
        "  * $L$ is the loss function\n",
        "\n",
        "* The aim is to find parameters $\\theta$ such that the model performs well on the **observed data**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 **2. Testing a Dataset (Test Set)**\n",
        "\n",
        "> **Definition**: The **test set** is a separate subset of data used to **evaluate** the model after training.\n",
        "> It is **not seen during training**, and thus provides an **unbiased estimate** of how the model will perform on **new, unseen data**.\n",
        "\n",
        "#### **Theoretical Purpose**:\n",
        "\n",
        "* Measures the **generalization ability** of the model.\n",
        "* Allows computation of evaluation metrics like **accuracy**, **precision**, **recall**, etc.\n",
        "* Reflects the **expected performance** on the true data distribution.\n",
        "\n",
        "$$\n",
        "\\text{Generalization Error} = \\mathbb{E}_{(x, y) \\sim P_{\\text{data}}} [L(f_\\theta(x), y)]\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Axiz6K4AKRoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "- In machine learning, preprocessing is the process of preparing raw data for a machine learning model. This typically involves transforming data into a format suitable for learning algorithms to perform well. The sklearn.preprocessing module in scikit-learn provides a set of tools that help with preprocessing tasks such as scaling, encoding, and normalizing data."
      ],
      "metadata": {
        "id": "0RL4paSvLzTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "- n machine learning, the test set is a subset of data that is used to evaluate the performance of a trained model. It is distinct from the training set, which is used to train the model. The test set plays a critical role in understanding how well a model can generalize to new, unseen data."
      ],
      "metadata": {
        "id": "vMO12z58L7Wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        " - Certainly! Here's a theoretical explanation for both of your questions:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. How do we split data for model fitting (training and testing)?**\n",
        "\n",
        "In machine learning, splitting data is essential to evaluate a model’s ability to generalize to unseen data. Typically, the available dataset is divided into at least two parts:\n",
        "\n",
        "#### **Training Set**\n",
        "\n",
        "* This subset is used to train the machine learning model.\n",
        "* The model learns the patterns and relationships in the data from this set.\n",
        "\n",
        "#### **Testing Set**\n",
        "\n",
        "* This subset is used to evaluate the model after it has been trained.\n",
        "* It simulates how the model will perform on real-world or future data.\n",
        "* No learning takes place on this data; it is purely for validation.\n",
        "\n",
        "#### **Why split the data?**\n",
        "\n",
        "* To avoid overfitting: A model might perform very well on the data it was trained on but poorly on new, unseen data.\n",
        "* To get an unbiased estimate of model performance.\n",
        "\n",
        "#### **Common Ratios**\n",
        "\n",
        "* **80/20** or **70/30** splits are common (e.g., 80% training, 20% testing).\n",
        "* In some cases, data is further split into three parts: **training**, **validation**, and **testing** (commonly used in deep learning).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How do you approach a Machine Learning problem?**\n",
        "\n",
        "Approaching a machine learning problem involves several structured steps, each critical to building an effective model. Here's the theoretical process:\n",
        "\n",
        "#### **Step 1: Define the Problem**\n",
        "\n",
        "* Understand the business or scientific objective.\n",
        "* Determine the type of machine learning task: classification, regression, clustering, etc.\n",
        "\n",
        "#### **Step 2: Collect and Understand the Data**\n",
        "\n",
        "* Gather data from relevant sources.\n",
        "* Understand the nature, size, and structure of the dataset.\n",
        "\n",
        "#### **Step 3: Prepare the Data**\n",
        "\n",
        "* Clean the data: handle missing values, outliers, and inconsistencies.\n",
        "* Transform features: encode categorical variables, scale numerical data, and create new features if needed.\n",
        "\n",
        "#### **Step 4: Explore the Data (Exploratory Data Analysis - EDA)**\n",
        "\n",
        "* Analyze patterns, correlations, and distributions.\n",
        "* Visualize the data to understand relationships between features and the target variable.\n",
        "\n",
        "#### **Step 5: Split the Data**\n",
        "\n",
        "* Divide the dataset into training and testing sets.\n",
        "* Optionally, use a validation set or cross-validation for model tuning.\n",
        "\n",
        "#### **Step 6: Select and Train a Model**\n",
        "\n",
        "* Choose a suitable algorithm based on the problem and data characteristics.\n",
        "* Train the model using the training set.\n",
        "\n",
        "#### **Step 7: Evaluate the Model**\n",
        "\n",
        "* Assess model performance using the testing set.\n",
        "* Use appropriate metrics (e.g., accuracy, F1-score, precision, recall, RMSE).\n",
        "\n",
        "#### **Step 8: Tune the Model**\n",
        "\n",
        "* Adjust hyperparameters to improve performance.\n",
        "* Use techniques like grid search or random search.\n",
        "\n",
        "#### **Step 9: Validate the Model**\n",
        "\n",
        "* Use cross-validation or a separate validation set to ensure the model generalizes well.\n",
        "\n",
        "#### **Step 10: Deploy and Monitor**\n",
        "\n",
        "* If the model meets the required performance, deploy it to production.\n",
        "* Monitor its performance over time and update it as necessary.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mPR9Ms9sMDxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        " - We perform **Exploratory Data Analysis (EDA)** before fitting a machine learning model because it helps us understand the **structure, quality, and relationships** in the dataset. Without EDA, we risk applying models blindly, which can lead to poor performance, incorrect conclusions, or misleading results.\n",
        "\n",
        "### ⚙️ **Key Reasons to Perform EDA Before Model Fitting:**\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Understand Data Structure**\n",
        "\n",
        "* Identify the types of features (numerical, categorical, text, dates, etc.).\n",
        "* Know the shape of the dataset (number of rows and columns).\n",
        "* Detect duplicated records or irrelevant columns.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Identify Missing or Inconsistent Data**\n",
        "\n",
        "* Check for null values or incorrect formats.\n",
        "* Decide how to handle missing data: impute, remove, or leave as-is.\n",
        "* Spot inconsistencies (e.g., \"Yes\", \"Y\", and \"1\" in the same column).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Detect Outliers and Noise**\n",
        "\n",
        "* Outliers can distort model training, especially for regression models.\n",
        "* EDA helps detect these through visual tools like boxplots or statistical summaries.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Understand Feature Distributions**\n",
        "\n",
        "* Know how each feature is distributed (normal, skewed, etc.).\n",
        "* This informs preprocessing choices like normalization or transformation.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Reveal Relationships Between Variables**\n",
        "\n",
        "* Correlation analysis shows how features relate to each other and the target.\n",
        "* Helps in identifying redundant or highly informative features.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Uncover Class Imbalance**\n",
        "\n",
        "* In classification tasks, it's common for one class to dominate.\n",
        "* EDA can expose imbalances that need to be addressed before modeling (e.g., with resampling techniques).\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Guide Feature Engineering**\n",
        "\n",
        "* EDA gives insight into useful transformations or combinations of variables.\n",
        "* For example, converting timestamps to day of week, or creating interaction terms.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Prevent Modeling Mistakes**\n",
        "\n",
        "* Without EDA, we may feed poor-quality data into the model.\n",
        "* This can lead to overfitting, underfitting, or invalid predictions.\n"
      ],
      "metadata": {
        "id": "DP7FJN4v5C1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "   Correlation is a statistical measure that expresses the strength and direction of a relationship between two variables.\n",
        "\n",
        "In simpler terms, it tells us:\n",
        "\n",
        "Whether two variables move together.\n",
        "\n",
        "How strongly they move together.\n",
        "\n",
        "In which direction (positive or negative) they move together.\n",
        "\n"
      ],
      "metadata": {
        "id": "lBtA9glf5QuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        " - ### **What Does Negative Correlation Mean?**\n",
        "\n",
        "A **negative correlation** means that **as one variable increases, the other decreases**, and vice versa.\n",
        "\n",
        "In other words, the two variables move in **opposite directions**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 **Example of Negative Correlation:**\n",
        "\n",
        "* **Outside temperature and heating bills**\n",
        "  As the **temperature increases**, your **heating bill usually decreases**.\n",
        "\n",
        "* **Speed and travel time**\n",
        "  The **faster** you drive, the **less time** it takes to reach your destination.\n",
        "\n",
        "---\n",
        "\n",
        "### 📐 **Negative Correlation Coefficient:**\n",
        "\n",
        "* A **correlation coefficient** (often denoted as **r**) measures the strength and direction of the relationship between two variables.\n",
        "* For a **negative correlation**, the value of **r is less than 0** (r < 0).\n"
      ],
      "metadata": {
        "id": "pQYxiQb95b1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        " -  The most common method is the Pearson correlation, which measures the linear relationship between continuous variables.\n",
        "\n",
        "✅ 1. Using Pandas .corr() Method\n",
        "Pandas makes it easy to compute correlations between numeric columns of a DataFrame."
      ],
      "metadata": {
        "id": "C2u8BjON5xOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "data = {\n",
        "    'height': [150, 160, 170, 180, 190],\n",
        "    'weight': [50, 60, 65, 80, 85]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMeVCZA557HP",
        "outputId": "cc3cd62c-00ed-4b0b-9990-75853908d87f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          height    weight\n",
            "height  1.000000  0.987878\n",
            "weight  0.987878  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 2. Correlation Between Two Specific Columns"
      ],
      "metadata": {
        "id": "WqD5FF2B6A4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_value = df['height'].corr(df['weight'])\n",
        "print(f\"Correlation between height and weight: {corr_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjb97Fzh6D3a",
        "outputId": "d28a51b2-899f-4c28-dd15-59057cde657c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between height and weight: 0.9878783399072132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 3. Visualizing Correlation Matrix with Seaborn\n",
        "Visualizations help detect strong or weak relationships at a glance."
      ],
      "metadata": {
        "id": "loVNOZ2E6F2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "IXej-jRb6IJP",
        "outputId": "f5a00eec-3819-4fde-ad86-6dd2e6f51036"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 4. Using NumPy\n",
        "If you're working with arrays instead of DataFrames:"
      ],
      "metadata": {
        "id": "XkXCW7pn6LTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([10, 9, 8, 7, 6])\n",
        "\n",
        "correlation = np.corrcoef(x, y)\n",
        "print(correlation)  # Outputs a 2x2 matrix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2pBj4HA6N1Q",
        "outputId": "ca047af6-d0b4-42ab-e251-0ce47f924db6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1. -1.]\n",
            " [-1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example\n",
        "\n",
        "- Causation (also called causality) means that one event or variable directly causes a change in another.\n",
        "\n",
        "If A causes B, then changes in A will directly produce changes in B.\n",
        "\n",
        "It's a cause-and-effect relationship.\n",
        "\n",
        "🔁 Difference Between Correlation and Causation\n",
        "Aspect\tCorrelation\tCausation\n",
        "Definition\tA relationship where two variables move together\tOne variable directly affects another\n",
        "Directionality\tNo cause-effect implied\tClear cause-effect relationship\n",
        "Proof Needed\tSimple statistics (e.g., Pearson correlation)\tRequires experiments or deep analysis\n",
        "Example Type\tObservational\tExperimental or controlled studies\n",
        "Misinterpretation Risk\tHigh — correlation is often mistaken for causation\tLower, if proven through robust methods\n",
        "\n",
        "📊 Example: Correlation ≠ Causation\n",
        "Scenario:\n",
        "You notice that ice cream sales and drowning incidents are positively correlated.\n",
        "\n",
        "Observation:\n",
        "When ice cream sales go up, drowning incidents also increase.\n",
        "\n",
        "Does ice cream cause drowning?\n",
        "No! This is a correlation, not causation.\n",
        "\n",
        "What's the real cause?\n",
        "A third variable — hot weather (summer) — causes both:\n",
        "\n",
        "More people buy ice cream.\n",
        "\n",
        "More people swim, increasing the risk of drowning.\n",
        "\n",
        "✅ So, while the variables are correlated, one does not cause the other.\n",
        "\n",
        "✅ When Can You Claim Causation?\n",
        "To confidently say one variable causes another, you usually need:\n",
        "\n",
        "Controlled experiments (e.g., A/B testing, randomized trials).\n",
        "\n",
        "Temporal evidence (cause happens before the effect).\n",
        "\n",
        "Elimination of confounding variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "RVR-G-pf6QmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        " - Certainly! Here's a **theoretical explanation** of what an **optimizer** is, and the different types:\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 **What is an Optimizer in Theory?**\n",
        "\n",
        "An **optimizer** is a mathematical function or algorithm used in machine learning to **adjust model parameters** (like weights and biases) in order to **minimize (or sometimes maximize) a loss function**.\n",
        "\n",
        "* The **loss function** tells us how far off our model's predictions are from the actual results.\n",
        "* The **optimizer** guides how the model should change to reduce that error, helping it learn better.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Goal of an Optimizer:**\n",
        "\n",
        "To **find the best values** for model parameters that result in the **lowest possible error** (or cost).\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **Types of Optimizers (Theoretical Overview)**\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Gradient Descent (GD)**\n",
        "\n",
        "#### 📝 Theory:\n",
        "\n",
        "* This is the most basic optimization algorithm.\n",
        "* It works by calculating the **gradient (slope)** of the loss function with respect to each parameter.\n",
        "* Parameters are updated by **moving in the opposite direction of the gradient** (i.e., downhill).\n",
        "\n",
        "#### 🔄 Process:\n",
        "\n",
        "1. Compute the gradient (partial derivatives).\n",
        "2. Update parameters using the gradient.\n",
        "\n",
        "#### 📌 Limitation:\n",
        "\n",
        "* Requires the **entire dataset** to compute gradients for one update, which is slow and computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "#### 📝 Theory:\n",
        "\n",
        "* Instead of using the full dataset, SGD updates model parameters **after each training example**.\n",
        "* This makes it much **faster** and **suitable for large datasets**.\n",
        "\n",
        "#### 📌 Limitation:\n",
        "\n",
        "* Updates can be **noisy** and cause the optimization path to oscillate.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Mini-Batch Gradient Descent**\n",
        "\n",
        "#### 📝 Theory:\n",
        "\n",
        "* A middle-ground between GD and SGD.\n",
        "* Updates model parameters using a **small batch of data points** at a time (e.g., 32 or 64 samples).\n",
        "* It combines the **stability** of GD and the **efficiency** of SGD.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Momentum**\n",
        "\n",
        "#### 📝 Theory:\n",
        "\n",
        "* Momentum adds a **\"memory\" of past gradients** to the update process.\n",
        "* Instead of using only the current gradient, it also considers previous updates to build **velocity**.\n",
        "\n",
        "#### 📌 Benefit:\n",
        "\n",
        "* Helps smooth the updates.\n",
        "* Allows the model to move faster through flat or noisy regions and avoid getting stuck in small local minima.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. AdaGrad (Adaptive Gradient Algorithm)**\n",
        "\n",
        "#### 📝 Theory:\n",
        "\n",
        "* Modifies the learning rate for each parameter **based on how frequently it's updated**.\n",
        "* Parameters that are updated frequently get **smaller learning rates**, and less frequently updated parameters get **larger learning rates**.\n",
        "\n",
        "#### 📌 Limitation:\n",
        "\n",
        "* Learning rates can shrink too much, causing the algorithm to stop learning.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. RMSProp (Root Mean Square Propagation)**\n",
        "\n",
        "#### 📝 Theory:\n",
        "\n",
        "* Improves upon AdaGrad by using a **moving average** of squared gradients.\n",
        "* Prevents the learning rate from shrinking too much over time.\n",
        "\n",
        "#### 📌 Benefit:\n",
        "\n",
        "* Works well in **non-stationary** problems (where data patterns change).\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "#### 📝 Theory:\n",
        "\n",
        "* Combines the advantages of **Momentum** and **RMSProp**.\n",
        "* Keeps track of both:\n",
        "\n",
        "  * The **average of past gradients** (momentum)\n",
        "  * The **average of past squared gradients** (adaptive learning rate)\n",
        "\n",
        "#### 📌 Benefits:\n",
        "\n",
        "* Widely used and very effective.\n",
        "* **Automatically adjusts** learning rates for different parameters.\n",
        "* Works well in most deep learning applications.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nHkRaiaJ6ieK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "\n",
        " - sklearn.linear_model is a module in Scikit-learn (a popular Python machine learning library) that provides classes and functions for implementing linear models—a fundamental category of machine learning algorithms used for both regression and classification tasks."
      ],
      "metadata": {
        "id": "wEbmWnZq7C3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        " - What does model.fit() do?\n",
        "In Scikit-learn, the fit() method is used to train a model on the provided data. It takes the training data and adjusts the model's internal parameters (such as weights and biases) to learn from the patterns in the data. Essentially, it \"fits\" the model to the data by finding the best parameters that minimize the error (or loss function).\n",
        "\n",
        "✅ Arguments that must be given to fit()\n",
        "X (features):\n",
        "\n",
        "A 2D array (or matrix) representing the input data. Each row corresponds to one sample, and each column corresponds to a feature.\n",
        "\n",
        "Shape: (n_samples, n_features) where n_samples is the number of data points and n_features is the number of input variables (attributes).\n",
        "\n",
        "y (target labels):\n",
        "\n",
        "A 1D array or vector containing the target variable values (the label/output for each sample).\n",
        "\n",
        "Shape: (n_samples,) for regression (continuous values) or classification (class labels), or (n_samples, n_classes) for multi-output tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "nYqC-lGw7OeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        " - The `model.predict()` method in Scikit-learn is used to generate predictions based on a model that has already been trained using the `model.fit()` method. After fitting the model to a dataset (which involves learning from input features and corresponding target labels), `predict()` takes new, unseen input data (features) and returns the model's predictions for those inputs. The method requires only the input features `X`, which should be a 2D array where each row represents a new sample and each column represents a feature. The shape of `X` should match the number of features used during training. Depending on the type of model, the output of `predict()` will vary: for regression models, it returns continuous numerical values, while for classification models, it outputs class labels (or probabilities if specified). This method is fundamental for applying the trained model to real-world data and generating predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fh0Ze0hF7f_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        " - ### **Continuous Variables:**\n",
        "\n",
        "Continuous variables are those that can take any value within a **range** and can be measured with infinite precision. They represent quantities that can be divided into smaller parts, allowing for a broad range of possible values.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * **Height** (e.g., 5.6 feet, 5.67 feet, 5.567 feet, etc.)\n",
        "  * **Weight** (e.g., 70.2 kg, 70.25 kg, etc.)\n",
        "  * **Temperature** (e.g., 23.5°C, 23.51°C, etc.)\n",
        "  * **Age** (e.g., 25.5 years, 25.56 years, etc.)\n",
        "\n",
        "These variables are typically measured on a scale and are usually expressed as **real numbers**. Continuous data can be represented by **real numbers** or **floating-point values**, and you can perform arithmetic operations (such as addition, subtraction, multiplication, etc.) on them.\n",
        "\n",
        "---\n",
        "\n",
        "### **Categorical Variables:**\n",
        "\n",
        "Categorical variables, also called **qualitative variables**, represent types or categories. These variables can take on a limited, fixed number of possible values, and each value represents a distinct category or group. They cannot be measured on a numerical scale, and the operations you can perform on them are limited to things like counting or grouping, rather than arithmetic.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * **Gender** (e.g., male, female, non-binary)\n",
        "  * **Marital Status** (e.g., single, married, divorced)\n",
        "  * **Country** (e.g., USA, India, Canada)\n",
        "  * **Color** (e.g., red, blue, green)\n",
        "\n",
        "Categorical variables can be divided into:\n",
        "\n",
        "1. **Nominal Variables**: Categories with no inherent order or ranking (e.g., color, country).\n",
        "2. **Ordinal Variables**: Categories with a defined order or ranking, but the distances between the ranks are not meaningful (e.g., education level: high school, bachelor’s, master’s, PhD).\n",
        "\n",
        "For categorical variables, you generally use **counting, frequency, or grouping** methods, rather than operations like addition or subtraction.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ut6qQPC08LbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        " - Feature scaling is the process of normalizing or standardizing the range of independent variables or features in a dataset. This ensures that all features contribute equally to the model and are on a comparable scale. Feature scaling is particularly important when working with algorithms that are sensitive to the scale of input data, such as gradient-based methods, distance-based algorithms like K-Nearest Neighbors (KNN) or Support Vector Machines (SVM), and neural networks.\n",
        "\n",
        "In simple terms, feature scaling adjusts the values of features so that they lie within a specific range or have similar statistical properties (like mean and variance), making it easier for machine learning algorithms to learn from the data effectively.\n",
        "\n",
        "Types of Feature Scaling\n",
        "There are two main techniques for feature scaling:\n",
        "\n",
        "Normalization (Min-Max Scaling):\n",
        "\n",
        "Definition: This method rescales the data so that all feature values lie within a specific range, typically between 0 and 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "norm\n",
        "=\n",
        "𝑋\n",
        "−\n",
        "min\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "max\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "−\n",
        "min\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "X\n",
        "norm\n",
        "​\n",
        " =\n",
        "max(X)−min(X)\n",
        "X−min(X)\n",
        "​\n",
        "\n",
        "When to use: It's useful when the data needs to be bounded within a specific range, especially for algorithms that rely on distance metrics (e.g., KNN, Neural Networks).\n",
        "\n",
        "Standardization (Z-Score Normalization):\n",
        "\n",
        "Definition: This method transforms the data to have a mean of 0 and a standard deviation of 1. It’s done by subtracting the mean and dividing by the standard deviation.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "std\n",
        "=\n",
        "𝑋\n",
        "−\n",
        "𝜇\n",
        "𝜎\n",
        "X\n",
        "std\n",
        "​\n",
        " =\n",
        "σ\n",
        "X−μ\n",
        "​\n",
        "\n",
        "where μ is the mean of the feature and σ is the standard deviation.\n",
        "\n",
        "When to use: This is useful when the data follows a Gaussian (normal) distribution and is often preferred for algorithms like linear regression, logistic regression, and SVMs.\n",
        "\n",
        "How Does Feature Scaling Help in Machine Learning?\n",
        "Prevents Dominance of Certain Features:\n",
        "\n",
        "When features have different scales, the model may give more importance to features with larger numerical ranges, as they dominate the optimization process. Feature scaling brings all features to a comparable scale, preventing such dominance and ensuring that each feature contributes equally.\n",
        "\n",
        "Improves Convergence in Gradient-Based Algorithms:\n",
        "\n",
        "Algorithms like Gradient Descent rely on iteratively updating the model’s parameters. If features have different scales, the gradient steps may be uneven, causing slow convergence or even making it difficult for the algorithm to converge at all. By scaling features, the convergence process becomes more stable and efficient.\n",
        "\n",
        "Helps in Distance-Based Algorithms:\n",
        "\n",
        "Algorithms such as K-Nearest Neighbors (KNN) or K-Means Clustering rely on calculating the distance between data points. If features are not scaled, the algorithm may be biased toward features with larger numerical ranges, making the distance metric misleading. Scaling ensures that all features contribute equally to the distance calculation.\n",
        "\n",
        "Improves Performance of Regularization:\n",
        "\n",
        "In algorithms that use regularization (e.g., Ridge or Lasso Regression), feature scaling is important to ensure that the penalty term (which discourages large coefficients) treats all features equally. Without scaling, the regularization may unfairly penalize features with smaller ranges.\n",
        "\n",
        "Enables Efficient Use of SVMs and Neural Networks:\n",
        "\n",
        "Support Vector Machines (SVMs) and neural networks rely heavily on distances and gradients. Scaling features helps to make training faster and more effective, as these models become sensitive to feature ranges, and unequal ranges may impair performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "cUC62FyG8axA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "\n",
        " - In Python, feature scaling can be easily performed using the **Scikit-learn** library, which provides tools like `StandardScaler` for standardization (z-score normalization) and `MinMaxScaler` for normalization (min-max scaling). Standardization involves transforming data so that each feature has a mean of 0 and a standard deviation of 1, which is useful for algorithms like linear regression, logistic regression, and neural networks. On the other hand, normalization scales the data into a fixed range, typically between 0 and 1, which is particularly helpful for distance-based algorithms like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM). To perform scaling, you first import the relevant scaler, then create an instance of it and use the `fit_transform()` method to scale the data. For instance, `StandardScaler` standardizes the features by subtracting the mean and dividing by the standard deviation, while `MinMaxScaler` scales the features by transforming them based on their minimum and maximum values. Additionally, feature scaling can be applied selectively to specific columns or used within a **pipeline** to automate the process, especially when handling other preprocessing tasks like missing value imputation. By performing feature scaling, you ensure that all features contribute equally to the machine learning model, which can improve model performance and convergence speed.\n"
      ],
      "metadata": {
        "id": "BJ9VA8b782Fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        " - sklearn.preprocessing is a module in the Scikit-learn library that provides a collection of functions and classes used for preprocessing data before feeding it into machine learning algorithms. It contains tools for transforming raw data into a format suitable for machine learning models, ensuring that the data is clean, scaled, and encoded properly.\n",
        "\n",
        "The preprocessing module includes a wide range of methods for scaling, normalizing, encoding, and imputing missing data. Proper preprocessing is crucial because many machine learning algorithms perform better or even require data to be in specific formats, such as standardized ranges or encoded categorical values."
      ],
      "metadata": {
        "id": "VjePV_rK9FWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        " - In Python, data splitting for model fitting is commonly done using Scikit-learn's train_test_split function, which allows you to divide your dataset into two subsets: one for training the model and another for testing its performance. This is crucial to evaluate the model’s generalizability and avoid overfitting.\n",
        "\n",
        "Steps to Split Data for Model Fitting in Python\n",
        "Import the Necessary Libraries:\n",
        "\n",
        "You'll need train_test_split from sklearn.model_selection to split the data, and typically numpy or pandas to handle the data.\n",
        "\n",
        "Prepare Your Dataset:\n",
        "\n",
        "You must have your data in the form of features (X) and target labels (y). Usually, X is a 2D array (features) and y is a 1D array (target labels).\n",
        "\n",
        "Use train_test_split:\n",
        "\n",
        "The train_test_split function splits the dataset into two sets: one for training (X_train, y_train) and one for testing (X_test, y_test).\n",
        "\n",
        "You can specify the test size (the proportion of the data to be used for testing) and the random state (to ensure reproducibility of the split).\n",
        "\n",
        "Example Code:\n",
        "\n"
      ],
      "metadata": {
        "id": "IlUdCcqB9QW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Feature matrix\n",
        "y = np.array([0, 1, 0, 1, 0])  # Target labels\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Features:\")\n",
        "print(X_train)\n",
        "print(\"Testing Features:\")\n",
        "print(X_test)\n",
        "print(\"Training Labels:\")\n",
        "print(y_train)\n",
        "print(\"Testing Labels:\")\n",
        "print(y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGkbNKJa9kx4",
        "outputId": "4a6885ca-efee-48ba-9b4f-b384d9addb02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features:\n",
            "[[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "Testing Features:\n",
            "[[3 4]]\n",
            "Training Labels:\n",
            "[0 0 0 1]\n",
            "Testing Labels:\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        " - Data encoding refers to the process of converting categorical data (non-numeric) into a numerical format, so that machine learning algorithms can work with it effectively. Most machine learning algorithms require numerical input to process the data, so encoding categorical variables is a crucial step in the data preprocessing pipeline. Encoding allows algorithms to interpret and use categorical information in a meaningful way.\n",
        "\n",
        "Categorical variables come in two types:\n",
        "\n",
        "Nominal: Categories without any specific order (e.g., color, country, gender).\n",
        "\n",
        "Ordinal: Categories with a meaningful order or ranking (e.g., education level: high school < bachelor's < master's).\n",
        "\n",
        "Common Methods of Data Encoding\n",
        "Label Encoding:\n",
        "\n",
        "Label Encoding converts each unique category in a categorical column into an integer label. For example, it will assign a unique integer to each category, like 0, 1, 2, etc.\n",
        "\n",
        "This method is typically used for ordinal data (where categories have a meaningful order).\n",
        "\n",
        "Limitations: Label encoding may impose an implicit ordinality (i.e., numerical distance between values like 0, 1, 2 might suggest some kind of ranking) even if the categorical variable doesn't have any inherent order.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iNCh173p9nGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Example data\n",
        "data = ['Low', 'Medium', 'High', 'Medium', 'Low']\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huWrU6NG95Ug",
        "outputId": "932d31b1-1d95-41f7-bafe-1168882b1ed2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 0 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding:\n",
        "\n",
        "One-Hot Encoding creates a binary column for each category in a categorical feature. For each observation, it places a 1 in the column corresponding to the category the observation belongs to and 0 in all other columns.\n",
        "\n",
        "This method is generally used for nominal data (where categories don’t have an inherent order).\n",
        "\n",
        "Advantages: It avoids implying any order, as each category is represented independently with its own column.\n",
        "\n",
        "Disadvantages: One-hot encoding increases the dimensionality of the dataset (known as the curse of dimensionality), especially when dealing with features that have many unique categories.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "UbwPCoEV98Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n",
        "\n",
        "# Perform one-hot encoding using pandas get_dummies\n",
        "one_hot_encoded = pd.get_dummies(data, columns=['Color'])\n",
        "\n",
        "print(one_hot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQszeySD-EZ4",
        "outputId": "c535943f-7bca-4b6e-975b-fe2c930a409b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Color_Blue  Color_Green  Color_Red\n",
            "0       False        False       True\n",
            "1        True        False      False\n",
            "2       False         True      False\n",
            "3        True        False      False\n",
            "4       False        False       True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinal Encoding:\n",
        "\n",
        "Ordinal Encoding is similar to label encoding but is specifically used when there is a meaningful order among the categories.\n",
        "\n",
        "It assigns an integer value to each category based on its position in the order.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "QgGs2MnO-Geg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Example data\n",
        "data = [['Low'], ['Medium'], ['High'], ['Medium'], ['Low']]\n",
        "\n",
        "# Initialize the OrdinalEncoder\n",
        "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46GkI_Xy-JoA",
        "outputId": "b77fefe8-78a5-420d-a12a-3a38646ee689"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Encoding:\n",
        "\n",
        "Binary Encoding is a combination of Hashing and One-Hot Encoding. It first assigns each category a unique integer, and then converts that integer into binary code. Each bit of the binary code becomes a separate feature.\n",
        "\n",
        "This is useful when there are many categories (high cardinality) and one-hot encoding becomes impractical due to the high dimensionality.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hbdpjdnf-N7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency or Count Encoding:\n",
        "\n",
        "Frequency Encoding or Count Encoding replaces each category with the frequency or count of that category in the dataset. This can be useful for high-cardinality categorical features and is a simple encoding technique.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "lAyS-pJg-hk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n",
        "\n",
        "# Frequency Encoding\n",
        "frequency_encoded = data['Color'].map(data['Color'].value_counts())\n",
        "\n",
        "print(frequency_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6SgtSF5-kZ5",
        "outputId": "66752dd5-422d-48f6-8f91-f2a65eb8d7fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    2\n",
            "1    2\n",
            "2    1\n",
            "3    2\n",
            "4    2\n",
            "Name: Color, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cycid3Hj-nZv"
      }
    }
  ]
}